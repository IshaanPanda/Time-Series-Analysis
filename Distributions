import scipy.stats as stats
import numpy as np
import matplotlib.pyplot as plt

# CDF and Quantile Functions
# Binomial distribution
stats.binom.pmf(5, 12, 0.5)
# stats
stats.binom.mean(12, 0.5)
stats.binom.std(12, 0.5)
stats.binom.var(12, 0.5)

# Normal distribution
stats.norm.cdf(0, loc = 0, scale = 1)
# Quantile function
stats.norm.ppf(0.5, loc = 0, scale = 1)
# generating random numbers
stats.norm.rvs(loc = 0, scale = 1, size = (2,3))

# What does it mean for a random variable to follow a distribution? If you *sample* from that distribution and plot a histogram of the samples, 
the shape of the histogram will resemble the pdf with large enough samples. Let's compare samples drawn from a standard normal distribution with samples 
drawn from a $t_5$ distribution.

x = stats.norm.rvs(size = 10000)
y = stats.t.rvs(df = 5, size = 10000)
plt.hist(x, bins = 50, alpha = 0.5, label = 'standard normal samples', density = True)
plt.hist(y, bins = 50, alpha = 0.5, label = 't_5 samples', density = True)
plt.legend()

Here's what a chi-square distribution looks like with increasing degrees of freedom

x = np.arange(0, 10, 0.01)
for k in range(1,10):
    plt.plot(x, stats.chi2.pdf(x, df = k), label = '%d degrees of freedom' %k)
plt.ylim((0, 1))
plt.legend()

You can create a random variable $T$ having a $t$-distribution with $n$ degrees of freedom. We need two other random variables for this:
- $Z \sim N(0,1)$
- $X \sim \Chi^2_n$
Then, $T = \frac{Z}{\sqrt{X/n}}$. Let's verify this computationally.

n = 5
z = stats.norm.rvs(size = 10000)
x = stats.chi2.rvs(df = n, size = 10000)
t = z / np.sqrt(x/n)
plt.hist(t, bins = 50, density=True)
plt.scatter(t, stats.t.pdf(t, df = n), s = 0.2, color='red')

As you can see above, the histogram of our constructed variable lines up perfectly with the pdf of a $t$-distribution.

To inspect the pdf of an $F$ distribution, we need to specify the numerator and denominator degrees of freedom.

k1 = 4; k2 = 3
x = np.arange(0, 10, 0.01)
plt.plot(x, stats.f.pdf(x, dfn = k1, dfd = k2), label = 'num df: %d, denom df: %d'%(k1, k2))
plt.legend()

#### Demonstrating Law of Large Numbers
Suppose $X_1, X_2, X_3, \ldots$ are Bernoulli random variables with $P[X_i = 1] = 0.5$. Let's construct three new random variables as follows:
$$Y_1 = \frac{X_1 + X_2 + \cdots + X_{10}}{10}$$
$$Y_2 = \frac{X_1 + X_2 + \cdots + X_{50}}{50}$$
$$Y_3 = \frac{X_1 + X_2 + \cdots + X_{500}}{500}$$
What do the distributions of $Y_1$, $Y_2$ and $Y_3$ look like? For each of these, we will perform 1000 simulations, giving us 1000 possible values for each of $Y_1$, $Y_2$, and $Y_3$. 
Then, we'll plot a histogram of these values and compare.

sample_y1 = [stats.bernoulli.rvs(p = 0.5, size = 10).mean() for _ in range(1000)]
sample_y2 = [stats.bernoulli.rvs(p = 0.5, size = 50).mean() for _ in range(1000)]
sample_y3 = [stats.bernoulli.rvs(p = 0.5, size = 500).mean() for _ in range(1000)]

plt.hist(sample_y1, bins = 20, label = 'Y1 Samples')
plt.hist(sample_y2, bins = 20, label = 'Y2 Samples')
plt.hist(sample_y3, bins = 20, label = 'Y3 Samples')
plt.legend()

Application: If $X$ follows a chi-squared distribution with $n$ degrees of freedom, then as $n$ gets large, the ratio $X/n$ starts to concentrate around 1.
Here's what that looks like:

for n in range(10, 200, 10):
    x = stats.chi2.rvs(df = n, size = 10000)
    plt.hist(x/n, bins = 50, label = 'n = %d'%n, density = True)
plt.xlim((0,5))
plt.legend()
